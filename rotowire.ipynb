{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d74224aa-c876-45be-bce6-395f7af6b740",
   "metadata": {},
   "source": [
    "# RotoWire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2fedd2-b6af-4a34-8771-b3fa711e8f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "from more_itertools import collapse\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from evals import BleuScore\n",
    "from utils import *\n",
    "from beam_search import beam_search\n",
    "from early_stopping import EarlyStopping\n",
    "\n",
    "# root directory\n",
    "root_dir = './rotowire'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea2fb2e-ed47-4786-a8be-59f75e4c57ba",
   "metadata": {},
   "source": [
    "## data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d083bd58-38ba-42c4-9ffc-deb68bed9248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "# reference: https://github.com/KaijuML/data-to-text-hierarchical/blob/master/data/make-dataset.py\n",
    "\n",
    "DELIMITER = \"ï¿¨\"\n",
    "ENTITY_SIZE = 24  # at most 24 elements in an entity\n",
    "\n",
    "# box_score keys\n",
    "bs_keys = ['START_POSITION', 'MIN', 'PTS', 'FGM', 'FGA', 'FG_PCT', 'FG3M',\n",
    "           'FG3A', 'FG3_PCT', 'FTM', 'FTA', 'FT_PCT', 'OREB', 'DREB', 'REB',\n",
    "           'AST', 'TO', 'STL', 'BLK', 'PF', 'FIRST_NAME', 'SECOND_NAME']\n",
    "# line_score keys\n",
    "ls_keys = ['PTS_QTR1', 'PTS_QTR2', 'PTS_QTR3', 'PTS_QTR4', 'PTS', 'FG_PCT',\n",
    "           'FG3_PCT', 'FT_PCT', 'REB', 'AST', 'TOV', 'WINS', 'LOSSES',\n",
    "           'CITY', 'NAME']\n",
    "ls_keys = [f'TEAM-{key}' for key in ls_keys]\n",
    "\n",
    "\n",
    "def build_home(entry):\n",
    "    \"\"\"The team who hosted the game\"\"\"\n",
    "    records = [DELIMITER.join(['<ent>', '<ent>'])]\n",
    "    for key in ls_keys:\n",
    "        records.append(DELIMITER.join([\n",
    "            entry['home_line'][key].replace(' ', '_'),\n",
    "            key\n",
    "        ]))\n",
    "\n",
    "    # Contrary to previous work, IS_HOME is now a unique token at the end\n",
    "    records.append(DELIMITER.join(['yes', 'IS_HOME']))\n",
    "\n",
    "    # We pad the entity to size ENT_SIZE with OpenNMT <blank> token\n",
    "    records.extend([DELIMITER.join(['<blank>', '<blank>'])]\n",
    "                   * (ENTITY_SIZE - len(records)))\n",
    "    return records\n",
    "\n",
    "\n",
    "def build_vis(entry):\n",
    "    \"\"\"The visiting team\"\"\"\n",
    "    records = [DELIMITER.join(['<ent>', '<ent>'])]\n",
    "    for key in ls_keys:\n",
    "        records.append(DELIMITER.join([\n",
    "            entry['vis_line'][key].replace(' ', '_'),\n",
    "            key\n",
    "        ]))\n",
    "\n",
    "    # Contrary to previous work, IS_HOME is now a unique token at the end\n",
    "    records.append(DELIMITER.join(['no', 'IS_HOME']))\n",
    "\n",
    "    # We pad the entity to size ENT_SIZE with OpenNMT <blank> token\n",
    "    records.extend([DELIMITER.join(['<blank>', '<blank>'])]\n",
    "                   * (ENTITY_SIZE - len(records)))\n",
    "    return records\n",
    "\n",
    "\n",
    "def get_player_idxs(entry):\n",
    "    # In 4 instances the Clippers play against the Lakers\n",
    "    # Both are from LA... We simply devide in half the players\n",
    "    # In all 4, there are 26 players so we return 13-25 & 0-12\n",
    "    # as it is always visiting first and home second.\n",
    "    if entry['home_city'] == entry['vis_city']:\n",
    "        assert entry['home_city'] == 'Los Angeles'\n",
    "        return ([str(idx) for idx in range(13, 26)],\n",
    "                [str(idx) for idx in range(13)])\n",
    "\n",
    "    nplayers = len(entry['box_score']['PTS'])\n",
    "    home_players, vis_players = list(), list()\n",
    "    for i in range(nplayers):\n",
    "        player_city = entry['box_score']['TEAM_CITY'][str(i)]\n",
    "        if player_city == entry['home_city']:\n",
    "            home_players.append(str(i))\n",
    "        else:\n",
    "            vis_players.append(str(i))\n",
    "    return home_players, vis_players\n",
    "\n",
    "\n",
    "def box_preprocess(entry, remove_na=True):\n",
    "    home_players, vis_players = get_player_idxs(entry)\n",
    "\n",
    "    all_entities = list()  # will contain all records of the input table\n",
    "\n",
    "    for is_home, player_idxs in enumerate([vis_players, home_players]):\n",
    "        for player_idx in player_idxs:\n",
    "            player = [DELIMITER.join(['<ent>', '<ent>'])]\n",
    "            for key in bs_keys:\n",
    "                val = entry['box_score'][key][player_idx]\n",
    "                if remove_na and val == 'N/A':\n",
    "                    continue\n",
    "                player.append(DELIMITER.join([\n",
    "                    val.replace(' ', '_'),\n",
    "                    key\n",
    "                ]))\n",
    "            is_home_str = 'yes' if is_home else 'no'\n",
    "            player.append(DELIMITER.join([is_home_str, 'IS_HOME']))\n",
    "\n",
    "            # We pad the entity to size ENT_SIZE with OpenNMT <blank> token\n",
    "            player.extend([DELIMITER.join(['<blank>', '<blank>'])]\n",
    "                          * (ENTITY_SIZE - len(player)))\n",
    "            all_entities.append(player)\n",
    "\n",
    "    all_entities.append(build_home(entry))\n",
    "    all_entities.append(build_vis(entry))\n",
    "    return list(collapse(all_entities))\n",
    "\n",
    "\n",
    "def clean_summary(summary, tokens):\n",
    "    \"\"\"\n",
    "    In here, we slightly help the copy mechanism\n",
    "    When we built the source sequence, we took all multi-words value\n",
    "    and repalaced spaces by underscores. We replace those as well in\n",
    "    the summaries, so that the copy mechanism knows it was a copy.\n",
    "    It only happens with city names like \"Los Angeles\".\n",
    "    \"\"\"\n",
    "    summary = ' '.join(summary)\n",
    "    for token in tokens:\n",
    "        val = token.split(DELIMITER)[0]\n",
    "        if '_' in val:\n",
    "            val_no_underscore = val.replace('_', ' ')\n",
    "            summary = summary.replace(val_no_underscore, val)\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050a44f9-124d-43b7-8878-e70f6013624d",
   "metadata": {},
   "source": [
    "## build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ab23bb-5fb7-420c-aa4a-cd0359d87783",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, summaries = [], []\n",
    "for setname in ['train', 'valid', 'test']:\n",
    "    filename = f'{root_dir}/data/{setname}.json'\n",
    "    with open(filename, encoding='utf8', mode='r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for entry in data:\n",
    "        input = box_preprocess(entry)\n",
    "        inputs.append(' '.join(input))\n",
    "        summaries.append(clean_summary(entry['summary'], input))\n",
    "\n",
    "keys = ls_keys + bs_keys\n",
    "keys.extend(['<blank>', '<ent>', 'IS_HOME'])\n",
    "keys.sort()\n",
    "\n",
    "field2ind = dict(zip(keys, [i for i in range(1, len(keys) + 1)]))\n",
    "field2ind['<pad>'] = 0\n",
    "\n",
    "word2ind = {}\n",
    "word2ind['<pad>'] = 0\n",
    "word2ind['<unk>'] = 1\n",
    "word2ind['<sos>'] = 2\n",
    "word2ind['<eos>'] = 3\n",
    "idx = 4\n",
    "\n",
    "# dataset distribution\n",
    "max_field, max_len = 0, 0\n",
    "text_len_dict = {idx: 0 for idx in range(0, 850)}\n",
    "\n",
    "for input in inputs:\n",
    "    items = input.split()\n",
    "    max_field = max(max_field, len(items))\n",
    "    for item in items:\n",
    "        w = item.split(DELIMITER)[0]\n",
    "        if w not in word2ind:\n",
    "            word2ind[w] = idx\n",
    "            idx += 1\n",
    "\n",
    "for summary in summaries:\n",
    "    _words = summary.split()\n",
    "    max_len = max(max_len, len(_words))\n",
    "    text_len_dict[len(_words)] += 1\n",
    "    for w in _words:\n",
    "        if w not in word2ind:\n",
    "            word2ind[w] = idx\n",
    "            idx += 1\n",
    "\n",
    "ind2word = {key: value for (value, key) in word2ind.items()}\n",
    "ind2field = {key: value for (value, key) in field2ind.items()}\n",
    "vocab = {'word2ind': word2ind, 'field2ind': field2ind}\n",
    "with open(f'{root_dir}/data/vocab.json', 'w', encoding='utf8') as f:\n",
    "    json.dump(vocab, f)\n",
    "\n",
    "print(f'max_cnt of field: {max_field}')  # 768\n",
    "print(f'max_len of summary: {max_len}')  # 813\n",
    "\n",
    "# get distribution of the length of summary\n",
    "\n",
    "plt.bar(list(text_len_dict.keys()), text_len_dict.values())\n",
    "plt.xticks([idx for idx in range(200, 850, 50)])\n",
    "plt.title('distribution of summary length')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad44f4db-2282-4011-9617-7efe7e4d422a",
   "metadata": {},
   "source": [
    "## data processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fefc12-24f2-44b6-988b-f2db99dedd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vocab_size = len(word2ind)  # 11473\n",
    "field_vocab_size = len(field2ind)  # 41\n",
    "\n",
    "max_field, max_len = 770, 700\n",
    "\n",
    "PAD_TOKEN = 0\n",
    "UNK_TOKEN = 1\n",
    "SOS_TOKEN = 2\n",
    "EOS_TOKEN = 3\n",
    "\n",
    "\n",
    "def _load_data(path):\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        data = json.load(f)\n",
    "    inputs, summaries = [], []\n",
    "\n",
    "    for entry in data:\n",
    "        input = box_preprocess(entry)\n",
    "        inputs.append(' '.join(input))\n",
    "        summaries.append(clean_summary(entry['summary'], input))\n",
    "    samples = []\n",
    "    for input, summary in zip(inputs, summaries):\n",
    "        samples.append({'input': input, 'summary': summary})\n",
    "    return samples\n",
    "\n",
    "\n",
    "def _process_data(data):\n",
    "    \"\"\"\n",
    "    process data to model-friendly format, i.e.\n",
    "    input: (max_field, 2) output: (max_len, 1)\n",
    "    \"\"\"\n",
    "    seq_info = np.zeros((len(data), max_field, 2))  # PAD\n",
    "    seq_target = np.zeros((len(data), max_len))  # PAD\n",
    "    for data_index, data_item in enumerate(data):\n",
    "        input_items = data_item['input'].split()\n",
    "        for idx, input_item in enumerate(input_items):\n",
    "            v, f = input_item.split(DELIMITER)\n",
    "            seq_info[data_index, idx, 0] = field2ind[f]\n",
    "            seq_info[data_index, idx,\n",
    "                     1] = word2ind[v] if v in word2ind else UNK_TOKEN\n",
    "        tokens_text = data_item['summary'].strip().split()\n",
    "        seq_target[data_index, 0] = 2  # SOS\n",
    "        for idx, token in enumerate(tokens_text):\n",
    "            if (idx + 1) < max_len:\n",
    "                seq_target[data_index, idx +\n",
    "                           1] = word2ind[token] if token in word2ind else UNK_TOKEN\n",
    "            else:\n",
    "                break\n",
    "    return seq_info, seq_target\n",
    "\n",
    "\n",
    "def process_one_data(idx_data, tag: str):\n",
    "    \"\"\"process data to model-friendly format one-by-one for test set.\"\"\"\n",
    "    seq_info = np.zeros((max_field, 2))  # PAD\n",
    "    if tag == 'test':\n",
    "        input_items = test_data[idx_data]['input'].split()\n",
    "    elif tag == 'dev':\n",
    "        input_items = dev_data[idx_data]['input'].split()\n",
    "    else:\n",
    "        ValueError('illegal tag: ', tag)\n",
    "    for idx, input_item in enumerate(input_items):\n",
    "        v, f = input_item.split(DELIMITER)\n",
    "        seq_info[idx, 0] = field2ind[f]\n",
    "        seq_info[idx, 1] = word2ind[v] if v in word2ind else UNK_TOKEN\n",
    "    return seq_info\n",
    "\n",
    "\n",
    "def get_data_loader(seq_info, seq_target, batch_size, shuffle, device):\n",
    "    seq_info_tensor = torch.tensor(seq_info, dtype=torch.long, device=device)\n",
    "    seq_target_tensor = torch.tensor(\n",
    "        seq_target, dtype=torch.long, device=device)\n",
    "    data_loader = Data.DataLoader(dataset=Data.TensorDataset(seq_info_tensor, seq_target_tensor),\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=shuffle)\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "def get_refs():\n",
    "    \"\"\" get gold summaries \"\"\"\n",
    "    list_refs = []\n",
    "    for data_item in test_data:\n",
    "        list_refs.append(data_item['summary'])\n",
    "    return list_refs\n",
    "\n",
    "\n",
    "def translate(list_seq):\n",
    "    \"\"\" translate sequence-in-numbers to real sentence \"\"\"\n",
    "    list_token = []\n",
    "    for index in list_seq:\n",
    "        if index == UNK_TOKEN:\n",
    "            continue\n",
    "        elif index == SOS_TOKEN:\n",
    "            continue\n",
    "        elif index == PAD_TOKEN:\n",
    "            continue\n",
    "        elif index == EOS_TOKEN:\n",
    "            break\n",
    "        else:\n",
    "            list_token.append(ind2word[index])\n",
    "    return ' '.join(list_token)\n",
    "\n",
    "\n",
    "def translate_with_copy(list_seq, attn_score, data_idx):\n",
    "    \"\"\" translate sequence-in-numbers to real sentence with copy mechanism \"\"\"\n",
    "    list_token = []\n",
    "    for index, attn in zip(list_seq, attn_score):\n",
    "        if index == UNK_TOKEN:\n",
    "            attn_max = attn.max(0)[1].item()\n",
    "            if attn_max < len(test_data[data_idx]['input'].split()):\n",
    "                list_token.append(test_data[data_idx]['input'].split()[\n",
    "                                  attn_max].split(DELIMITER)[0])\n",
    "            else:\n",
    "                list_token.append('<unk>')\n",
    "        elif index == SOS_TOKEN:\n",
    "            continue\n",
    "        elif index == PAD_TOKEN:\n",
    "            continue\n",
    "        elif index == EOS_TOKEN:\n",
    "            break\n",
    "        else:\n",
    "            list_token.append(ind2word[index])\n",
    "    return ' '.join(list_token)\n",
    "\n",
    "\n",
    "seq_info_train, seq_target_train = _process_data(\n",
    "    _load_data(f'{root_dir}/data/train.json'))\n",
    "seq_info_dev, seq_target_dev = _process_data(\n",
    "    _load_data(f'{root_dir}/data/valid.json'))\n",
    "dev_data = _load_data(f'{root_dir}/data/valid.json')\n",
    "test_data = _load_data(f'{root_dir}/data/test.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f996ca0-da16-4a16-84b7-13ed96dbed5b",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e28c320-d90f-4d83-94cc-df76c80e12fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderAttn(nn.Module):\n",
    "    def __init__(self, field_vocab_size, word_vocab_size, embed_dim_field, embed_dim_word, hidden_dim, dropout_p):\n",
    "        super(EncoderAttn, self).__init__()\n",
    "        self.embed_dim_field = embed_dim_field\n",
    "        self.embed_dim_word = embed_dim_word\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.field_embedding = nn.Embedding(field_vocab_size, embed_dim_field)\n",
    "        self.value_embedding = nn.Embedding(word_vocab_size, embed_dim_word)\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        self.bi_lstm = nn.LSTM(input_size=embed_dim_field + embed_dim_word,\n",
    "                               hidden_size=hidden_dim,\n",
    "                               batch_first=True,\n",
    "                               bias=True,\n",
    "                               bidirectional=True)\n",
    "        self.fc_lstm = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.weight_reply = nn.Parameter(torch.randn(\n",
    "            hidden_dim, hidden_dim), requires_grad=True)\n",
    "        self.fc_gate = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.layernorm_gate = nn.LayerNorm(hidden_dim, elementwise_affine=True)\n",
    "        self.layernorm_lstm = nn.LayerNorm(hidden_dim, elementwise_affine=True)\n",
    "\n",
    "    def _content_selection_gate(self, reply_encoder):\n",
    "        # reply_encoder(batch, record_num, hid_dim), reply_post(batch, hid_dim, record_num)\n",
    "        reply_post = reply_encoder.permute(0, 2, 1)\n",
    "        alpha = F.softmax(torch.matmul(\n",
    "            torch.matmul(reply_encoder, self.weight_reply), reply_post), dim=2)\n",
    "        reply_c = torch.bmm(alpha, reply_encoder)\n",
    "        attn_gate = torch.sigmoid(self.layernorm_gate(\n",
    "            self.fc_gate(torch.cat((reply_encoder, reply_c), dim=2))))\n",
    "        reply_cs = attn_gate * reply_encoder\n",
    "        return reply_cs\n",
    "\n",
    "    def forward(self, encoder_input):\n",
    "        field_embed = self.field_embedding(encoder_input[:, :, 0])\n",
    "        value_embed = self.value_embedding(encoder_input[:, :, 1])\n",
    "        field_value = self.dropout(\n",
    "            torch.cat((field_embed, value_embed), dim=2))\n",
    "\n",
    "        encoder_output, (h_n, c_n) = self.bi_lstm(field_value)\n",
    "        encoder_output = self.layernorm_lstm((self.fc_lstm(encoder_output)))\n",
    "        h_n = self.layernorm_lstm(self.fc_lstm(\n",
    "            torch.cat((h_n[-2], h_n[-1]), dim=1)))\n",
    "        c_n = self.layernorm_lstm(self.fc_lstm(\n",
    "            torch.cat((c_n[-2], c_n[-1]), dim=1)))\n",
    "        content_selection = self._content_selection_gate(encoder_output)\n",
    "\n",
    "        return content_selection, (h_n, c_n)\n",
    "\n",
    "\n",
    "class DecoderAttn(nn.Module):\n",
    "    def __init__(self, word_vocab_size, embed_dim, hidden_dim, dropout_p):\n",
    "        super(DecoderAttn, self).__init__()\n",
    "        self.output_dim = word_vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(word_vocab_size, embed_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        self.lstm = nn.LSTMCell(embed_dim + hidden_dim, hidden_dim, bias=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, word_vocab_size)\n",
    "\n",
    "    @staticmethod\n",
    "    def _weighted_encoder_rep(decoder_hidden, content_selection):\n",
    "        energy = (decoder_hidden.unsqueeze(1) * content_selection) / \\\n",
    "            math.sqrt(decoder_hidden.size(-1))\n",
    "        attn_score = F.softmax(torch.sum(energy, dim=2), dim=1)\n",
    "        attn_with_selector = attn_score.unsqueeze(dim=2) * content_selection\n",
    "        return torch.sum(attn_with_selector, dim=1), attn_score\n",
    "\n",
    "    def forward(self, decoder_input, decoder_hidden, content_selection):\n",
    "        embed = self.dropout(self.embedding(decoder_input))\n",
    "        attn_vector, attn_score = self._weighted_encoder_rep(\n",
    "            decoder_hidden[0], content_selection)\n",
    "        emb_attn_combine = torch.cat((embed, attn_vector), dim=1)\n",
    "        h_n, c_n = self.lstm(emb_attn_combine, decoder_hidden)\n",
    "        decoder_output = F.log_softmax(self.fc_out(h_n), dim=1)\n",
    "\n",
    "        return decoder_output, (h_n, c_n), attn_score\n",
    "\n",
    "\n",
    "class Table2Text(nn.Module):\n",
    "    def __init__(self, encoder, decoder, beam_width, max_len, max_field):\n",
    "        super(Table2Text, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.beam_width = beam_width\n",
    "        self.max_len = max_len\n",
    "        self.max_field = max_field\n",
    "\n",
    "    def forward(self, seq_input, seq_target, train_mode):\n",
    "        # encoder\n",
    "        content_selection, decoder_hidden = self.encoder(seq_input)\n",
    "\n",
    "        if train_mode:\n",
    "            batch_size = seq_target.size(0)\n",
    "            seq_output = torch.zeros(\n",
    "                (batch_size, self.max_len, self.decoder.output_dim)).cuda()\n",
    "            for timeStep in range(self.max_len):\n",
    "                decoder_input = seq_target[:, timeStep]\n",
    "                decoder_output, decoder_hidden, _ = self.decoder(\n",
    "                    decoder_input, decoder_hidden, content_selection)\n",
    "                seq_output[:, timeStep, :] = decoder_output\n",
    "            return seq_output\n",
    "        else:\n",
    "            if self.beam_width == 1:  # beam search with beam_width=1 equals to greedy search\n",
    "                attn_map = torch.zeros((self.max_len, self.max_field)).cuda()\n",
    "                seq_output = torch.zeros(self.max_len).cuda()\n",
    "                decoder_input = seq_target  # first token: SOS_TOKEN\n",
    "                for timeStep in range(self.max_len):\n",
    "                    decoder_output, decoder_hidden, attn_score = self.decoder(\n",
    "                        decoder_input, decoder_hidden, content_selection)\n",
    "                    decoder_input = decoder_output.max(1)[1]\n",
    "                    seq_output[timeStep] = decoder_input.squeeze()\n",
    "                    attn_map[timeStep] = attn_score.squeeze()\n",
    "                    if decoder_input.item() == 3:  # EOS_TOKEN\n",
    "                        attn_map = attn_map[:timeStep]\n",
    "                        break\n",
    "            else:  # beam search\n",
    "                seq_output, attn_map = beam_search(max_len=self.max_len,\n",
    "                                                   max_field=self.max_field,\n",
    "                                                   beam_width=self.beam_width,\n",
    "                                                   decoder=self.decoder,\n",
    "                                                   decoder_input=seq_target,\n",
    "                                                   decoder_hidden=decoder_hidden,\n",
    "                                                   content_selection=content_selection)\n",
    "            return seq_output, attn_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226b729b-8183-4ced-b1f6-69691ec833b8",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d45f4f3-6743-44da-8530-1d792383d494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(experiment_time, model, optimizer):\n",
    "    check_file_exist(f'{root_dir}/results/checkpoints')\n",
    "    checkpoint_path = f'{root_dir}/results/checkpoints/' + \\\n",
    "        experiment_time + '.pth'\n",
    "    checkpoint = {\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "\n",
    "def load_checkpoint(latest, file_name=None):\n",
    "    \"\"\" load the latest checkpoint \"\"\"\n",
    "    checkpoints_dir = f'{root_dir}/results/checkpoints'\n",
    "    if latest:\n",
    "        file_list = os.listdir(checkpoints_dir)\n",
    "        file_list.sort(key=lambda fn: os.path.getmtime(\n",
    "            checkpoints_dir + '/' + fn))\n",
    "        checkpoint = torch.load(checkpoints_dir + '/' + file_list[-1])\n",
    "        return checkpoint, str(file_list[-1])\n",
    "    else:\n",
    "        if file_name is None:\n",
    "            raise ValueError('checkpoint_path cannot be empty!')\n",
    "        checkpoint = torch.load(checkpoints_dir + '/' + file_name)\n",
    "        return checkpoint, file_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d78892f-2f79-41af-bc85-dda52484c697",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4e1b6a-bc3d-41bc-b5c1-94376cef905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "batch_size = 32\n",
    "max_epoch = 50\n",
    "lr = 3e-4\n",
    "field_emb_dim = 32\n",
    "word_emb_dim = 300\n",
    "hidden_dim = 512\n",
    "dropout = 0.1\n",
    "random_seed = 1\n",
    "beam_width = 1\n",
    "train = True\n",
    "resume = False\n",
    "copy = False\n",
    "\n",
    "fix_seed(random_seed)\n",
    "\n",
    "cur_time = datetime.datetime.now().strftime('%Y_%m_%d_%H_%M')\n",
    "logger = get_logger(f'{root_dir}/results/logs/' + cur_time + '.log')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train_data_loader = get_data_loader(\n",
    "    seq_info_train, seq_target_train, batch_size, True, device)\n",
    "dev_data_loader = get_data_loader(\n",
    "    seq_info_dev, seq_target_dev, batch_size, False, device)\n",
    "\n",
    "logger.info(f'data processing consumes: {(time.time() - start_time):.2f}s')\n",
    "\n",
    "encoder = EncoderAttn(field_vocab_size, word_vocab_size,\n",
    "                      field_emb_dim, word_emb_dim, hidden_dim, dropout)\n",
    "decoder = DecoderAttn(word_vocab_size, word_emb_dim, hidden_dim, dropout)\n",
    "\n",
    "model = Table2Text(encoder, decoder, beam_width, max_len, max_field).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.NLLLoss()\n",
    "early_stop = EarlyStopping(mode='min', min_delta=0.001, patience=5)\n",
    "\n",
    "if resume:\n",
    "    checkpoint, cp_name = load_checkpoint(latest=True)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    logger.info(f'load checkpoint: [{cp_name}]')\n",
    "\n",
    "\n",
    "def train(data_loader):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for train_input, train_target in tqdm(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        train_output = model(train_input, train_target, train_mode=True)\n",
    "        train_output = train_output[:, :-1].reshape(-1, train_output.size(-1))\n",
    "        loss = criterion(train_output, train_target[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    return epoch_loss / len(data_loader)\n",
    "\n",
    "\n",
    "def validate(data_loader):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for dev_input, dev_target in data_loader:\n",
    "            dev_output = model(dev_input, dev_target, train_mode=True)\n",
    "            dev_output = dev_output[:, :-1].reshape(-1, dev_output.size(-1))\n",
    "            loss = criterion(dev_output, dev_target[:, 1:].reshape(-1))\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)\n",
    "\n",
    "\n",
    "def evaluate(infer_input):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        infer_target = torch.tensor(\n",
    "            [SOS_TOKEN], dtype=torch.long, device=device)\n",
    "        eval_output, attn = model(infer_input, infer_target, train_mode=False)\n",
    "    return eval_output, attn\n",
    "\n",
    "\n",
    "loss_dict_train, loss_dict_dev = [], []\n",
    "for epoch in range(1, int(max_epoch + 1)):\n",
    "    start_time = time.time()\n",
    "    train_loss = train(train_data_loader)\n",
    "    dev_loss = validate(dev_data_loader)\n",
    "    loss_dict_train.append(train_loss)\n",
    "    loss_dict_dev.append(dev_loss)\n",
    "\n",
    "    epoch_min, epoch_sec = record_time(start_time, time.time())\n",
    "    logger.info(\n",
    "        f'epoch: [{epoch:02}/{max_epoch}]  train_loss={train_loss:.3f}  valid_loss={dev_loss:.3f}  '\n",
    "        f'duration: {epoch_min}m {epoch_sec}s')\n",
    "\n",
    "    if early_stop.step(dev_loss):\n",
    "        logger.info(f'early stop at [{epoch:02}/{max_epoch}]')\n",
    "        break\n",
    "\n",
    "if max_epoch > 0:\n",
    "    save_checkpoint(experiment_time=cur_time, model=model, optimizer=optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Use the standard script provided by nltk.\"\"\"\n",
    "bleu_scorer = BleuScore()\n",
    "bleu_scorer.set_refs(get_refs())\n",
    "ie_metrics_list = []\n",
    "\n",
    "for idx_data in range(len(test_data)):\n",
    "    seq_input = torch.tensor(process_one_data(\n",
    "        idx_data, 'test'), dtype=torch.long, device=device).unsqueeze(0)\n",
    "    seq_output, attn_map = evaluate(seq_input)\n",
    "    list_seq = seq_output.squeeze().tolist()\n",
    "    if not copy:\n",
    "        text_gen = translate(list_seq)\n",
    "    else:\n",
    "        text_gen = translate_with_copy(\n",
    "            list_seq=list_seq, attn_score=attn_map, data_idx=idx_data)\n",
    "    bleu_scorer.add_gen(text_gen)\n",
    "bleu_score = bleu_scorer.calculate()\n",
    "logger.info(f'bleu score: {bleu_score:.2f}')\n",
    "\n",
    "for idx_data in range(len(dev_data)):\n",
    "    seq_input = torch.tensor(process_one_data(\n",
    "        idx_data, 'dev'), dtype=torch.long, device=device).unsqueeze(0)\n",
    "    seq_output, attn_map = evaluate(seq_input)\n",
    "    list_seq = seq_output.squeeze().tolist()\n",
    "    if not copy:\n",
    "        text_gen = translate(list_seq)\n",
    "    else:\n",
    "        text_gen = translate_with_copy(\n",
    "            list_seq=list_seq, attn_score=attn_map, data_idx=idx_data)\n",
    "    ie_metrics_list.append(text_gen)\n",
    "\n",
    "# generate summaries\n",
    "with open(f'{root_dir}/results/roto_cc-beam5_gens.txt', 'w', encoding='utf-8') as f:\n",
    "    for text in ie_metrics_list:\n",
    "        f.write(text + '\\n')\n",
    "\n",
    "\"\"\"\n",
    "Next step is the *Information/Relation Extraction.*\n",
    "\n",
    "Reference: [harvardnlp/data2text](https://github.com/harvardnlp/data2text)\n",
    "\n",
    "There are some minor bugs in the `data_utils.py` running with python3, but fix will be easy.\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "76205315a7514dc3e21d0994517b8c3fc247e0dabb2de8b66e07be1cdaae552f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('torch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
